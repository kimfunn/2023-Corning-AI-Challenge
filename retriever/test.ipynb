{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='bert-base-uncased'\n",
    "POOLER_TYPE='cls'\n",
    "TEMP = 1\n",
    "DATA_TYPE = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('THUDM/webglm-qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer_kwargs = {\"use_fast\": 'use_fast_tokenizer'}\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simcse import SimCSE\n",
    "# simcse_model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data[DATA_TYPE]\n",
    "queries, answers, references = test_dataset['question'], test_dataset['answer'], test_dataset['references']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [' '.join(x) for x in references]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_q = tokenizer(queries, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "encoding_d = tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "\n",
    "x_q, mask_q = encoding_q['input_ids'].to(device), encoding_q['attention_mask'].to(device)\n",
    "x_d, mask_d = encoding_d['input_ids'].to(device), encoding_d['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## # If you use AutoModel\n",
    "output_q = model(x_q, mask_q)\n",
    "output_d = model(x_d, mask_d)\n",
    "\n",
    "## # If you use SimCSE\n",
    "# output_q = simcse_model.encode(queries)\n",
    "# output_d = simcse_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    \n",
    "    def __init__(self, pooler_type):\n",
    "        super().__init__()\n",
    "        self.pooler_type = pooler_type\n",
    "        assert self.pooler_type in [\"cls\", \"cls_before_pooler\", \"max\", \"avg\", \"avg_top2\", \"avg_first_last\"], \"unrecognized pooling type %s\" % self.pooler_type\n",
    "\n",
    "    def forward(self, attention_mask, outputs):\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        pooler_output = outputs.pooler_output\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        if self.pooler_type in ['cls_before_pooler', 'cls']:\n",
    "            return last_hidden[:, 0]\n",
    "        elif self.pooler_type == \"avg\":\n",
    "            return ((last_hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1))\n",
    "        elif self.pooler_type == \"max\":\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "            )\n",
    "            last_hidden[input_mask_expanded == 0] = -1e9\n",
    "            max_over_time = torch.max(last_hidden, 1)[0]\n",
    "            return max_over_time\n",
    "        elif self.pooler_type == \"avg_first_last\":\n",
    "            first_hidden = hidden_states[1]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "            return pooled_result\n",
    "        elif self.pooler_type == \"avg_top2\":\n",
    "            second_last_hidden = hidden_states[-2]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "            return pooled_result\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "class Similarity(nn.Module):\n",
    "    \n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use AutoModel\n",
    "pooler = Pooler(POOLER_TYPE)\n",
    "z_q = pooler(mask_q, output_q)\n",
    "z_d = pooler(mask_d, output_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Similarity(TEMP)\n",
    "# cos_sim = sim(z_q.unsqueeze(1), z_d.unsqueeze(0)) # If you use AutoModel\n",
    "cos_sim = sim(output_q.unsqueeze(1), output_d.unsqueeze(0)) # If you use SimCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def topk_metric(matrix, k=None):\n",
    "    result = []\n",
    "    for c in matrix:\n",
    "        topk = sorted(c, reverse=True)[:k]\n",
    "        diag = np.diag(matrix)\n",
    "        is_topk = any(x in topk for x in np.diag(matrix))\n",
    "        result.append(0 if is_topk else 1) # 정답이면 0 아니면 1\n",
    "    return result\n",
    "\n",
    "result_lst = topk_metric(cos_sim.cpu().detach().numpy(), k=30)\n",
    "acc = result_lst.count(0) / len(result_lst) * 100\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "closed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
